{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Word embeddings are numerical representations of words in a high-dimensional vector space that capture semantic relationships and contextual information. These embeddings are essential in natural language processing (NLP) tasks, enabling computers to understand and process textual data more effectively. One common method to create word embeddings is Word2Vec, which utilizes neural networks to map words to dense vectors in such a way that similar words are close together in the vector space. Another popular approach is the use of pre-trained embeddings like Word2Vec, GloVe, or FastText, which are learned on large text corpora and can be fine-tuned for specific NLP tasks. Additionally, TF-IDF (Term Frequency-Inverse Document Frequency) can be used to create embeddings by representing the importance of words in documents relative to a corpus. These embeddings are valuable for various NLP tasks, such as sentiment analysis, text classification, machine translation, and more, as they provide a compact and meaningful representation of words that captures their semantic properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/naseeha/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/naseeha/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/naseeha/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/naseeha/.local/lib/python3.10/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/naseeha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"The Cricket World Cup is a major event in the world of sports.\\n\"+\n",
    "    \"Cricket players from different countries participate in the World Cup.\\n\"+\n",
    "    \"The final match of the World Cup was an exciting game.\\n\"+\n",
    "    \"Batsmen scored heavily in the tournament.\\n\"+\n",
    "    \"The World Cup trophy is awarded to the winning team.\\n\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "word_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in text:\n",
    "    x = [i.lower() for i in word_tokenize(sent) if i.isalpha()]\n",
    "    sentences.append(x)\n",
    "    for word in x:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    " \n",
    "#Set of vocab \n",
    "word_set = set(word_set)\n",
    "#Total documents in our corpus\n",
    "total_documents = len(sentences)\n",
    " \n",
    "#Creating an index for each word in our vocab.\n",
    "index_dict = {} #Dictionary to store index for each word\n",
    "i = 0\n",
    "for word in word_set:\n",
    "    index_dict[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a count dictionary\n",
    " \n",
    "def count_dict(sentences):\n",
    "    word_count = {}\n",
    "    for word in word_set:\n",
    "        word_count[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    " \n",
    "word_count = count_dict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    '''Increase the scalability of the embedding by taking the logarithm to the base 10 so that text corpus with 1000s or even more words work efficently'''\n",
    "    return np.log10(occurance/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverse Document Frequency\n",
    " \n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    '''Increase the scalability of the embedding by taking the logarithm to the base 10 so that text corpus with 1000s or even more words work efficently'''\n",
    "    return np.log10(total_documents/word_occurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(sentence):\n",
    "    tf_idf_vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[index_dict[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51144093 0.51144093 0.33020282 0.42082187 0.51144093 0.51144093\n",
      " 0.51144093 0.51144093 0.42082187 0.36781312 0.51144093 0.23958376\n",
      " 0.51144093 0.51144093 0.51144093 0.51144093 0.51144093 0.51144093\n",
      " 0.51144093 0.51144093 0.51144093 0.30103    0.51144093 0.51144093\n",
      " 0.51144093 0.51144093 0.51144093 0.51144093 0.51144093 0.51144093\n",
      " 0.42082187]\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Encoded text corpus\n",
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vec = tf_idf(sent)\n",
    "    vectors.append(vec)\n",
    " \n",
    "print(vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
