{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlAocOKN4Z2C",
        "outputId": "c4d28e36-48a4-48ac-d2bf-4b2ac95113a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYkH5y3Qzckz"
      },
      "source": [
        "# Translation Models\n",
        "\n",
        "\n",
        "Machine translation is a pivotal field within natural language processing (NLP) that focuses on automating the conversion of text or speech from one language to another. It relies on sophisticated models and techniques to accomplish this challenging task effectively. One of the cornerstone methods in machine translation is the sequence-to-sequence (seq2seq) model, which employs deep neural networks to encode input text and then decode it into the target language. This technique has revolutionized translation tasks by learning to capture complex linguistic nuances and contextual information. Additionally, other models like Transformer-based models, including the famous BERT and GPT-3, have also made significant strides in translation, leveraging attention mechanisms to excel in various language pairs and domains. The choice of model depends on specific translation requirements, language pairs, and the quality of available training data. In this Colab file, we havee given a basic demo on how tto use the dataset and work on a simple seq2seq moel usig RNN.Your task will be to improve the model to the maximum you can ,make prediction on the test dataset given and write a code to generate the BLEU score of you prediction compared to original.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Importing Libraries"
      ],
      "metadata": {
        "id": "s3U_EJucPXrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8nS1d9rgev8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout,Attention\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Loading and pre-processing the data"
      ],
      "metadata": {
        "id": "1qTHWrDtPdeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A) Loading the dataset"
      ],
      "metadata": {
        "id": "PgN5wPEJPqud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbtD0ux5ew7d"
      },
      "outputs": [],
      "source": [
        "#Loading and processing data\n",
        "eng_fr = pd.read_csv(\"/content/nlp_intel_train.csv\")\n",
        "ng_fr_test = pd.read_csv(\"/content/nlp_intel_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B) Dropping NaN texts"
      ],
      "metadata": {
        "id": "RONlgjFoPiaA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bs9CABoPfty1"
      },
      "outputs": [],
      "source": [
        "eng_fr = eng_fr.dropna(axis=0, how=\"any\", subset=None, inplace=False)\n",
        "eng_fr_test = eng_fr_test.dropna(axis=0, how=\"any\", subset=None, inplace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C) Removing non-alphabetic characters"
      ],
      "metadata": {
        "id": "NHT3gBJLP_49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: x.lower())\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: x.lower())\n",
        "\n",
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: re.sub(\"'\",\"\",x))\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: re.sub(\"'\",\"\",x))\n",
        "\n",
        "exclude=set(string.punctuation)\n",
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "\n",
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: re.sub(\"[1234567890]\",\"\",x))\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: re.sub(\"[1234567890]\",\"\",x))\n",
        "\n",
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: x.strip())\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: x.strip())\n",
        "\n",
        "eng_fr[\"en\"]=eng_fr[\"en\"].apply(lambda x: re.sub(\" +\",\" \",x))\n",
        "eng_fr[\"fr\"]=eng_fr[\"fr\"].apply(lambda x: re.sub(\" +\",\" \",x))\n",
        "\n",
        "eng_fr"
      ],
      "metadata": {
        "id": "Nh7t1En30U0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D) Making a list of all sentences"
      ],
      "metadata": {
        "id": "XYZm-Kc7QNmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XltalkhYO32R"
      },
      "outputs": [],
      "source": [
        "X=eng_fr[\"en\"].tolist()\n",
        "Y=eng_fr[\"fr\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E) Functions for pre-processing the sentences"
      ],
      "metadata": {
        "id": "9HrVfc8EQWgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #####  (i) Function to make a corpus from the available sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "XWDbf6gwQj9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzboP5K41eUD"
      },
      "outputs": [],
      "source": [
        "def to_corpus(sent_list):\n",
        "  text_corpus=\"\"\n",
        "  for sentence in sent_list:\n",
        "    text_corpus+=sentence.lower()+\" \"\n",
        "  return text_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (ii) Function to train the tokenizer"
      ],
      "metadata": {
        "id": "hClKUU9rQsMd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YJhb4zR3P3U"
      },
      "outputs": [],
      "source": [
        "def train_tokenizer(file_path):\n",
        "  tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  files=[file_path]\n",
        "  tokenizer.train(files, trainer)\n",
        "  tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\", pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),(\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),],)\n",
        "  return tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (iii) Function to form sequence of integers for each sentence"
      ],
      "metadata": {
        "id": "BMT13TCMQ1v0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLmqfOFO58wH"
      },
      "outputs": [],
      "source": [
        "def sequences(tokenizer,sent_list):\n",
        "  prepoc_sentences=[]\n",
        "  for sent in sent_list:\n",
        "    encoding=tokenizer.encode(sent.lower())\n",
        "    prepoc_sentences.append(encoding.ids)\n",
        "  prepoc_sentences = pad_sequences(prepoc_sentences,55, padding='post')\n",
        "  prepoc_sentences=np.array(prepoc_sentences)\n",
        "  return prepoc_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (iv) Function to form a dictionary of words"
      ],
      "metadata": {
        "id": "e7g71mUxSUWI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYKMV-13xOBl"
      },
      "outputs": [],
      "source": [
        "def vocabulary(path_vocab):\n",
        "    import json\n",
        "    f=open(path_vocab)\n",
        "    vocab=json.load(f)\n",
        "    dict_vocab={}\n",
        "    for i in vocab[\"model\"][\"vocab\"]:\n",
        "        dict_vocab[vocab[\"model\"][\"vocab\"][i]]= i\n",
        "    return dict_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F) Making a corpus out of the sentences"
      ],
      "metadata": {
        "id": "B_wmZnnIQ8IG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5scfSlq5Bbm",
        "outputId": "fedd8bad-9d1b-4e34-9d40-a9c5473a59e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2368969"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "f1=open(\"x.txt\",\"w\")\n",
        "f1.write(to_corpus(X))\n",
        "\n",
        "f2=open(\"y.txt\",\"w\")\n",
        "f2.write(to_corpus(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### G) Training and saving the tokenizer with the corpus made"
      ],
      "metadata": {
        "id": "x45gbNuURLoH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqPjHCho40SM"
      },
      "outputs": [],
      "source": [
        "tokenizer_eng,tokenizer_fr=train_tokenizer(\"/content/x.txt\"),train_tokenizer(\"/content/y.txt\")\n",
        "tokenizer_eng.save(\"english_vocab.json\",pretty=True)\n",
        "tokenizer_fr.save(\"french_vocab.json\",pretty=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H) Forming a sequence for each sentence"
      ],
      "metadata": {
        "id": "cMBVVzl1Rh9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_Zsr8Ql6uLf"
      },
      "outputs": [],
      "source": [
        "prepoc_english_sentences,prepoc_french_sentences=sequences(tokenizer_eng,X),sequences(tokenizer_fr,Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I) Creating a dictionary of words and ids"
      ],
      "metadata": {
        "id": "dsSDgmuwRy5n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhTjDKEVxOBl"
      },
      "outputs": [],
      "source": [
        "dict_eng,dict_fr=vocabulary(\"english_vocab.json\"),vocabulary(\"french_vocab.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Building and Training the se2seq model"
      ],
      "metadata": {
        "id": "pFZghHC7SlPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A) Building the model"
      ],
      "metadata": {
        "id": "p3nRMeGjS21X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "    learning_rate=0.003\n",
        "    model=Sequential()\n",
        "    model.add(Embedding(30000,256))\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))  #Bidirectional LSTM would be better\n",
        "    model.add(Dense(1024, activation='selu'))\n",
        "    model.add(Dropout(0.1))                                   #Adding an attention layer will also optimise the model\n",
        "    model.add(Bidirectional(GRU(256,return_sequences=True)))\n",
        "    model.add(Dense(1024, activation='selu'))\n",
        "    model.add(Dense(30000, activation='softmax'))\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "BPd1vLXi2QJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B) Training the model"
      ],
      "metadata": {
        "id": "AtlYKG5SS8FZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x = pad_sequences(prepoc_french_sentences, maxlen = 55, padding = 'post')\n",
        "\n",
        "\n",
        "# Train\n",
        "model = build_model()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(tmp_x, prepoc_english_sentences, batch_size=64, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7k5eev-qRkW",
        "outputId": "89c39bd7-6dbd-47b8-801f-8c2eb821d616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, None, 256)         7680000   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, None, 512)         789504    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, None, 1024)        525312    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, None, 1024)        0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, None, 512)         1969152   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, None, 1024)        525312    \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, None, 30000)       30750000  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 42239280 (161.13 MB)\n",
            "Trainable params: 42239280 (161.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "225/225 [==============================] - 91s 362ms/step - loss: 2.6489 - accuracy: 0.6728 - val_loss: 2.2006 - val_accuracy: 0.7072\n",
            "Epoch 2/20\n",
            "225/225 [==============================] - 77s 341ms/step - loss: 2.2078 - accuracy: 0.6904 - val_loss: 2.1107 - val_accuracy: 0.7107\n",
            "Epoch 3/20\n",
            "225/225 [==============================] - 76s 336ms/step - loss: 1.9513 - accuracy: 0.6988 - val_loss: 2.0976 - val_accuracy: 0.7110\n",
            "Epoch 4/20\n",
            "225/225 [==============================] - 74s 328ms/step - loss: 1.7268 - accuracy: 0.7070 - val_loss: 2.1459 - val_accuracy: 0.7051\n",
            "Epoch 5/20\n",
            "225/225 [==============================] - 72s 320ms/step - loss: 1.5282 - accuracy: 0.7199 - val_loss: 2.2048 - val_accuracy: 0.7055\n",
            "Epoch 6/20\n",
            "225/225 [==============================] - 71s 317ms/step - loss: 1.3777 - accuracy: 0.7330 - val_loss: 2.2487 - val_accuracy: 0.7010\n",
            "Epoch 7/20\n",
            "225/225 [==============================] - 74s 329ms/step - loss: 1.2687 - accuracy: 0.7446 - val_loss: 2.3180 - val_accuracy: 0.6986\n",
            "Epoch 8/20\n",
            "225/225 [==============================] - 74s 328ms/step - loss: 1.1601 - accuracy: 0.7570 - val_loss: 2.4090 - val_accuracy: 0.6919\n",
            "Epoch 9/20\n",
            "225/225 [==============================] - 74s 330ms/step - loss: 1.0593 - accuracy: 0.7698 - val_loss: 2.4877 - val_accuracy: 0.6896\n",
            "Epoch 10/20\n",
            "225/225 [==============================] - 74s 327ms/step - loss: 0.9754 - accuracy: 0.7818 - val_loss: 2.5548 - val_accuracy: 0.6895\n",
            "Epoch 11/20\n",
            "225/225 [==============================] - 71s 317ms/step - loss: 0.9057 - accuracy: 0.7927 - val_loss: 2.6699 - val_accuracy: 0.6914\n",
            "Epoch 12/20\n",
            "225/225 [==============================] - 74s 329ms/step - loss: 0.8384 - accuracy: 0.8035 - val_loss: 2.7336 - val_accuracy: 0.6885\n",
            "Epoch 13/20\n",
            "225/225 [==============================] - 73s 326ms/step - loss: 0.7815 - accuracy: 0.8129 - val_loss: 2.8042 - val_accuracy: 0.6887\n",
            "Epoch 14/20\n",
            "225/225 [==============================] - 74s 327ms/step - loss: 0.7306 - accuracy: 0.8230 - val_loss: 2.8867 - val_accuracy: 0.6915\n",
            "Epoch 15/20\n",
            "225/225 [==============================] - 74s 327ms/step - loss: 0.6835 - accuracy: 0.8315 - val_loss: 3.0246 - val_accuracy: 0.6810\n",
            "Epoch 16/20\n",
            "225/225 [==============================] - 73s 326ms/step - loss: 0.6419 - accuracy: 0.8395 - val_loss: 2.9984 - val_accuracy: 0.6868\n",
            "Epoch 17/20\n",
            "225/225 [==============================] - 71s 316ms/step - loss: 0.6100 - accuracy: 0.8466 - val_loss: 3.0879 - val_accuracy: 0.6888\n",
            "Epoch 18/20\n",
            "225/225 [==============================] - 73s 326ms/step - loss: 0.5775 - accuracy: 0.8535 - val_loss: 3.2429 - val_accuracy: 0.6802\n",
            "Epoch 19/20\n",
            "225/225 [==============================] - 74s 327ms/step - loss: 0.5458 - accuracy: 0.8598 - val_loss: 3.2940 - val_accuracy: 0.6840\n",
            "Epoch 20/20\n",
            "225/225 [==============================] - 73s 326ms/step - loss: 0.5195 - accuracy: 0.8659 - val_loss: 3.4346 - val_accuracy: 0.6844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a0ee0bfa560>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Predictions"
      ],
      "metadata": {
        "id": "nngrbVgfTBC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A) Function to Convert sequence to sentence"
      ],
      "metadata": {
        "id": "2QQ1aY2sTFIs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HarkZpzPxOBm"
      },
      "outputs": [],
      "source": [
        "def logits_to_text(logits,dict_lan):\n",
        "  while logits[-1]==0:\n",
        "    logits.pop(-1)\n",
        "  return ' '.join([dict_lan[prediction] for prediction in np.argmax(logits, 1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B) Prediction from training set"
      ],
      "metadata": {
        "id": "eiu42iqjTTcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i= 8\n",
        "\n",
        "print(\"Prediction:\")\n",
        "test_val=logits_to_text(model.predict(tmp_x[[i]])[0], dict_eng)\n",
        "print(test_val)\n",
        "print(\"\\nCorrect Translation:\")\n",
        "actual_val=X[i]\n",
        "print(X[i])\n",
        "print(\"\\nOriginal text:\")\n",
        "print(Y[i])"
      ],
      "metadata": {
        "id": "Ez8TfRzMr95J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Metrics"
      ],
      "metadata": {
        "id": "RJHrd4NfXp3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = test_val.split()\n",
        "reference = actual_val.split()\n",
        "#there may be several references\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
        "print(BLEUscore)\n"
      ],
      "metadata": {
        "id": "miFTGN6tZMlF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}