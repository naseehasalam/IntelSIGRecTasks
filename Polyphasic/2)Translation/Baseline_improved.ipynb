{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Models\n",
        "\n",
        "\n",
        "Machine translation is a pivotal field within natural language processing (NLP) that focuses on automating the conversion of text or speech from one language to another. It relies on sophisticated models and techniques to accomplish this challenging task effectively. One of the cornerstone methods in machine translation is the sequence-to-sequence (seq2seq) model, which employs deep neural networks to encode input text and then decode it into the target language. This technique has revolutionized translation tasks by learning to capture complex linguistic nuances and contextual information. Additionally, other models like Transformer-based models, including the famous BERT and GPT-3, have also made significant strides in translation, leveraging attention mechanisms to excel in various language pairs and domains. The choice of model depends on specific translation requirements, language pairs, and the quality of available training data. In this Colab file, we havee given a basic demo on how tto use the dataset and work on a simple seq2seq moel usig RNN.Your task will be to improve the model to the maximum you can ,make prediction on the test dataset given and write a code to generate the BLEU score of you prediction compared to original.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYkH5y3Qzckz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8nS1d9rgev8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece,BPE\n",
        "from tokenizers.trainers import WordPieceTrainer,BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Loading and processing data\n",
        "eng_fr = pd.read_csv(\"nlp_intel_train.csv\")\n",
        "eng_fr_test = pd.read_csv(\"nlp_intel_test.csv\")"
      ],
      "metadata": {
        "id": "nbtD0ux5ew7d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_fr = eng_fr.dropna(axis=0, how=\"any\", subset=None, inplace=False)\n",
        "eng_fr_test = eng_fr_test.dropna(axis=0, how=\"any\", subset=None, inplace=False)"
      ],
      "metadata": {
        "id": "Bs9CABoPfty1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=eng_fr[\"en\"].tolist()\n",
        "Y=eng_fr[\"fr\"].tolist()"
      ],
      "metadata": {
        "id": "XltalkhYO32R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_corpus(sent_list):\n",
        "  text_corpus=\"\"\n",
        "  for sentence in sent_list:\n",
        "    text_corpus+=sentence.lower()\n",
        "  return text_corpus"
      ],
      "metadata": {
        "id": "xzboP5K41eUD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(file_path):\n",
        "  tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  files=[file_path]\n",
        "  tokenizer.train(files, trainer)\n",
        "  tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\", pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),(\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),],)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "4YJhb4zR3P3U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences(tokenizer,sent_list):\n",
        "  prepoc_sentences=[]\n",
        "  for sent in sent_list:\n",
        "    encoding=tokenizer.encode(sent)\n",
        "    prepoc_sentences.append(encoding.ids)\n",
        "  prepoc_sentences=np.array(prepoc_sentences)\n",
        "  prepoc_sentences = pad_sequences(prepoc_sentences,55, padding='post')\n",
        "  return prepoc_sentences"
      ],
      "metadata": {
        "id": "bLmqfOFO58wH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1=open(\"x.txt\",\"w\")\n",
        "f1.write(to_corpus(X))\n",
        "\n",
        "f2=open(\"y.txt\",\"w\")\n",
        "f2.write(to_corpus(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5scfSlq5Bbm",
        "outputId": "f2d5bf7c-ab2b-49bf-da9a-2a522b1d1511"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2515115"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_eng,tokenizer_fr=train_tokenizer(\"/content/x.txt\"),train_tokenizer(\"/content/y.txt\")"
      ],
      "metadata": {
        "id": "XqPjHCho40SM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepoc_english_sentences,prepoc_french_sentences=sequences(tokenizer_eng,X),sequences(tokenizer_fr,Y)"
      ],
      "metadata": {
        "id": "r_Zsr8Ql6uLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}