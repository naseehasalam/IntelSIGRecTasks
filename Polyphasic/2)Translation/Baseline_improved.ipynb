{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGCERwNz1RoR",
        "outputId": "3500c8b1-5d17-4663-d3b0-fc7773e39494"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Models\n",
        "\n",
        "\n",
        "Machine translation is a pivotal field within natural language processing (NLP) that focuses on automating the conversion of text or speech from one language to another. It relies on sophisticated models and techniques to accomplish this challenging task effectively. One of the cornerstone methods in machine translation is the sequence-to-sequence (seq2seq) model, which employs deep neural networks to encode input text and then decode it into the target language. This technique has revolutionized translation tasks by learning to capture complex linguistic nuances and contextual information. Additionally, other models like Transformer-based models, including the famous BERT and GPT-3, have also made significant strides in translation, leveraging attention mechanisms to excel in various language pairs and domains. The choice of model depends on specific translation requirements, language pairs, and the quality of available training data. In this Colab file, we havee given a basic demo on how tto use the dataset and work on a simple seq2seq moel usig RNN.Your task will be to improve the model to the maximum you can ,make prediction on the test dataset given and write a code to generate the BLEU score of you prediction compared to original.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZYkH5y3Qzckz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8nS1d9rgev8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional,LSTM, Dropout\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece,BPE\n",
        "from tokenizers.trainers import WordPieceTrainer,BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Loading and processing data\n",
        "eng_fr = pd.read_csv(\"nlp_intel_train.csv\")\n",
        "eng_fr_test = pd.read_csv(\"nlp_intel_test.csv\")"
      ],
      "metadata": {
        "id": "nbtD0ux5ew7d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eng_fr = eng_fr.dropna(axis=0, how=\"any\", subset=None, inplace=False)\n",
        "eng_fr_test = eng_fr_test.dropna(axis=0, how=\"any\", subset=None, inplace=False)"
      ],
      "metadata": {
        "id": "Bs9CABoPfty1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=eng_fr[\"en\"].tolist()\n",
        "Y=eng_fr[\"fr\"].tolist()"
      ],
      "metadata": {
        "id": "XltalkhYO32R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_corpus(sent_list):\n",
        "  text_corpus=\"\"\n",
        "  for sentence in sent_list:\n",
        "    text_corpus+=sentence.lower()\n",
        "  return text_corpus"
      ],
      "metadata": {
        "id": "xzboP5K41eUD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_tokenizer(file_path):\n",
        "  tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "  trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  files=[file_path]\n",
        "  tokenizer.train(files, trainer)\n",
        "  tokenizer.post_processor = TemplateProcessing(single=\"[CLS] $A [SEP]\", pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",special_tokens=[(\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),(\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),],)\n",
        "  return tokenizer"
      ],
      "metadata": {
        "id": "4YJhb4zR3P3U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sequences(tokenizer,sent_list):\n",
        "  prepoc_sentences=[]\n",
        "  for sent in sent_list:\n",
        "    encoding=tokenizer.encode(sent)\n",
        "    prepoc_sentences.append(encoding.ids)\n",
        "  prepoc_sentences=np.array(prepoc_sentences)\n",
        "  prepoc_sentences = pad_sequences(prepoc_sentences,55, padding='post')\n",
        "  return prepoc_sentences"
      ],
      "metadata": {
        "id": "bLmqfOFO58wH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1=open(\"x.txt\",\"w\")\n",
        "f1.write(to_corpus(X))\n",
        "\n",
        "f2=open(\"y.txt\",\"w\")\n",
        "f2.write(to_corpus(Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5scfSlq5Bbm",
        "outputId": "f2d5bf7c-ab2b-49bf-da9a-2a522b1d1511"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2515115"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_eng,tokenizer_fr=train_tokenizer(\"/content/x.txt\"),train_tokenizer(\"/content/y.txt\")"
      ],
      "metadata": {
        "id": "XqPjHCho40SM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prepoc_english_sentences,prepoc_french_sentences=sequences(tokenizer_eng,X),sequences(tokenizer_fr,Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_Zsr8Ql6uLf",
        "outputId": "5cb5d219-24d1-41a7-ef7d-3349f3ab4df0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-98da483b1bb8>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  prepoc_sentences=np.array(prepoc_sentences)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''##Tokenizer and padding\n",
        "\n",
        "def tokenize(data):\n",
        "  t=Tokenizer()\n",
        "  t.fit_on_texts(data)\n",
        "  return t\n",
        "def training_sequences(tokenizer, m_length, data):\n",
        "    seq = tokenizer.texts_to_sequences(data)\n",
        "    seq = pad_sequences(seq, maxlen = m_length, padding='post')\n",
        "    return seq\n",
        "'''"
      ],
      "metadata": {
        "id": "cenL2y2df2p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#Preprocessing by tokenization and padding\n",
        "#return processed data and tokenizer\n",
        "def preprocess(x, y):\n",
        "\n",
        "    x_tk = tokenize(x)\n",
        "    y_tk = tokenize(y)\n",
        "\n",
        "    preprocess_x = training_sequences(x_tk,55,x)\n",
        "    preprocess_y = training_sequences(y_tk,55,y)\n",
        "\n",
        "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
        "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
        "\n",
        "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
        "'''"
      ],
      "metadata": {
        "id": "dKK20PUMg4K3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer = preprocess(eng_fr[\"en\"].tolist(), eng_fr[\"fr\"].tolist())'''"
      ],
      "metadata": {
        "id": "FWl-vgZziSeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''preproc_english_sentences[0]'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ng4up-wlvrra",
        "outputId": "add1bf86-e985-43d2-a976-94b2ccfc21df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    4,  3430,   241,  5895,     1,   904,  2716,     2, 11594,\n",
              "           4,  1306,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
        "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
        "english_vocab_size = len(english_tokenizer.word_index)\n",
        "french_vocab_size = len(french_tokenizer.word_index)\n",
        "\n",
        "print(\"Max English sentence length:\", max_english_sequence_length)\n",
        "print(\"Max French sentence length:\", max_french_sequence_length)\n",
        "print(\"English vocabulary size:\", english_vocab_size)\n",
        "print(\"French vocabulary size:\", french_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yU5F-7eii6N",
        "outputId": "ca6d056e-a7ae-41e3-c3db-ab48a6c1893e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max English sentence length: 55\n",
            "Max French sentence length: 55\n",
            "English vocabulary size: 21789\n",
            "French vocabulary size: 27712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Final output funtion\n",
        "def logits_to_text(logits, tokenizer):\n",
        "\n",
        "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
        "    index_to_words[0] = ' '\n",
        "\n",
        "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
      ],
      "metadata": {
        "id": "tP1THRqril3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
        "\n",
        "    learning_rate = 0.001\n",
        "\n",
        "    # Build the layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(french_vocab_size, 256, input_length=input_shape[1], input_shape=input_shape[1:]))\n",
        "    model.add(GRU(256, return_sequences=True))\n",
        "    model.add(Dense(1024, activation='relu'))\n",
        "    model.add(Dense(english_vocab_size, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=sparse_categorical_crossentropy,\n",
        "                  optimizer=Adam(learning_rate),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "Eoy5079ni0z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preproc_french_sentences.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bRXEU8TjWNL",
        "outputId": "2f87dfe0-52c2-447d-f16d-ad3802a180db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp_x =pad_sequences(preproc_french_sentences, maxlen = 55, padding = 'post')\n",
        "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
        "\n",
        "# Train\n",
        "model = bd_model(\n",
        "    tmp_x.shape,\n",
        "    preproc_english_sentences.shape[1],\n",
        "    len(english_tokenizer.word_index)+1,\n",
        "    len(french_tokenizer.word_index)+1)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit(tmp_x, preproc_english_sentences, batch_size=64, epochs=1, validation_split=0.2)"
      ],
      "metadata": {
        "id": "Bu1W91SAi1mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6c6309-658a-4095-d073-61f47f8a8309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 55, 256)           7094528   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 55, 256)           394752    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 55, 1024)          263168    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 55, 21790)         22334750  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30087198 (114.77 MB)\n",
            "Trainable params: 30087198 (114.77 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "225/225 [==============================] - 67s 275ms/step - loss: 3.0880 - accuracy: 0.6692 - val_loss: 2.4539 - val_accuracy: 0.6950\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b00fefe2ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i= 1\n",
        "\n",
        "\n",
        "print(\"Prediction:\")\n",
        "print(logits_to_text(model.predict(tmp_x[[i]])[0], english_tokenizer))\n",
        "print(\"\\nCorrect Translation:\")\n",
        "print(eng_fr[\"en\"].tolist()[i])\n",
        "print(\"\\nOriginal text:\")\n",
        "print(eng_fr[\"fr\"].tolist()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdY7DBVQksvf",
        "outputId": "2b083e54-4d42-4a07-aa8c-25ab45b55073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\n",
            "1/1 [==============================] - 0s 409ms/step\n",
            "the the the the the the the the the the                                                                                          \n",
            "\n",
            "Correct Translation:\n",
            "The club was very active and they twice organized the annual conference of the Amateur Astronomy Federation of Quebec in 1990 and 1997.\n",
            "\n",
            "Original text:\n",
            "Le club est très actif et organise à deux occasions (en 1990 et 1997) le congrès annuel de la Fédération des Astronomes Amateurs du Québec.\n"
          ]
        }
      ]
    }
  ]
}